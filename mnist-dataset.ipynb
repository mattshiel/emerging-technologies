{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the MNIST Dataset\n",
    "\n",
    "It is a dataset of images of handwritten digits. The dataset contains a training set of 60,000 examples, and a test set of 10,000 examples. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1.\n",
    "\n",
    "The handwritten digits themselves were written by high school students and employees of the United States Census Bureau; divided into Special Database 1 and Special Database 3 respectively; this combination of two NIST databases are what form the MNIST dataset.\n",
    "\n",
    "# Reading the MNIST Dataset \n",
    "\n",
    "In this section I'm going to be showing you how to read the MNIST dataset into memory from scratch. This is often done in one or two lines if using a library like Tensorflow or Keras eg.\n",
    "\n",
    "```\n",
    "from keras.datasets import mnist\n",
    "# get the dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "```\n",
    "\n",
    "Pre-configured data files are fantastic and they save a lot of time, however it's important that we know how to prepare and load databases ourselves.\n",
    "\n",
    "I tried to organize this Notebook into 4 easy to digest sections:\n",
    "\n",
    "    1. Download and extract the four MNIST g-zipped files\n",
    "    2. Process the encoded images\n",
    "    3. Save the images to a dictionary\n",
    "    4. Store dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Downloading and Extracting the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first task is to download the necessary files. When ran, the code below will create two folders **Data** and **MNISTData** in the current directory. From there we can use the `urllib` package for working with URLs, specifically `urllib.request` for opening and reading URLs.\n",
    "\n",
    "Now that we have a way to download the file, we need an easy way to name each file. The names for each file are contained in the URLs after the final forward slash. the `split` command coupled with negative index [-1] allows us to take what we need ie. _\"train-images-idx3-ubyte.gz\"_\n",
    "\n",
    "The final step is to download the data and copy it to our named file in the directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading  train-images-idx3-ubyte.gz\n",
      "Downloading  train-labels-idx1-ubyte.gz\n",
      "Downloading  t10k-images-idx3-ubyte.gz\n",
      "Downloading  t10k-labels-idx1-ubyte.gz\n",
      "All files are available\n"
     ]
    }
   ],
   "source": [
    "# Adapted from https://github.com/Ghosh4AI/Data-Processors/blob/master/MNIST/MNIST_Loader.ipynb\n",
    "\n",
    "import os,urllib.request\n",
    "\n",
    "# The path where the necessary files are downloaded to\n",
    "datapath = './Data/MNISTData/'  \n",
    "\n",
    "# Create the directory if it doesn't already exist\n",
    "if not os.path.exists(datapath):\n",
    "    os.makedirs(datapath)\n",
    "\n",
    "# The necessary download URLS for the training and testing images/labels\n",
    "urls = ['http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
    "       'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
    "       'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
    "       'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz']\n",
    "\n",
    "# Loop through each URL \n",
    "for url in urls:\n",
    "    # Get individual filenames from the list\n",
    "    filename = url.split('/')[-1]\n",
    "    \n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(datapath + filename):\n",
    "        print(filename, ' already exists')\n",
    "    else:\n",
    "        print('Downloading ', filename)\n",
    "        # Copy object specified in the URL to the local file\n",
    "        urllib.request.urlretrieve (url, datapath + filename)\n",
    "        \n",
    "print('All files are available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second part of preparing our now downloaded dataset is to extract the necessary files. The .idx files we need are all stored in the zipped .gz format. Using the `open` function provided by the `gzip` package, we can store the file object within as `file_in`, remove the .gz extension and copy `file_in` to a new `file_out` object without the .gz extension.\n",
    "\n",
    "Lastly we can clean up the .gz archives leftover so that we only have our meaningful data remaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting  t10k-images-idx3-ubyte.gz\n",
      "Extracting  train-images-idx3-ubyte.gz\n",
      "Extracting  train-labels-idx1-ubyte.gz\n",
      "Extracting  t10k-labels-idx1-ubyte.gz\n",
      "Extraction Complete\n",
      "\n",
      "Removing  t10k-images-idx3-ubyte.gz\n",
      "Removing  train-images-idx3-ubyte.gz\n",
      "Removing  train-labels-idx1-ubyte.gz\n",
      "Removing  t10k-labels-idx1-ubyte.gz\n",
      "All archives removed\n"
     ]
    }
   ],
   "source": [
    "import os,gzip,shutil\n",
    "\n",
    "# The path containing the MNIST data\n",
    "datapath = './Data/MNISTData/'  \n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(datapath)\n",
    "\n",
    "# Read each file\n",
    "for file in files:\n",
    "    if file.endswith('gz'):\n",
    "        print('Extracting ',file)   \n",
    "        # Open the folder and read the file to copy from\n",
    "        with gzip.open(datapath + file, 'rb') as file_in:\n",
    "            # Define the new file to copy to (read and drop the .gz extension)\n",
    "            with open(datapath + file.split('.')[0], 'wb') as file_out:\n",
    "                # copy the object contents to the new file\n",
    "                shutil.copyfileobj(file_in, file_out)\n",
    "print('Extraction Complete\\n')\n",
    "\n",
    "# Clean up and remove the gz folders, run twice to remove all files\n",
    "for file in files:\n",
    "    print('Removing ', file)\n",
    "    os.remove(datapath + file)\n",
    "print ('All archives removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Processing the Encoded Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our extracted files, we need a way to process the images and labels encoded within. The primary challenge here is to isolate the images from labels and seperate them into their respective sets of either **test** or **train**.\n",
    "\n",
    "- - -\n",
    "\n",
    "There are several components of the idx format that are important to understand. Firstly, the _magic number_.\n",
    "\n",
    "* **bytes 0-3:** Each file has a _magic number_, which is an integer. Integers in the files are stored in the MSB first (high endian) format. This integer will either be 2049, representing labels, or 2051, representing images. The first 32 bits (4 bytes) must therefore be converted to an integer to find out whether we're dealing with an image or a label.\n",
    "\n",
    "\n",
    "* **bytes 4-7:** The next 4 bytes provide us with another 32 bit integer, this time representing the amount of items/  or images eg. for t10k-labels-idx1-ubyte this will be 10,000 labels\n",
    "\n",
    "\n",
    "* **bytes 8-11:** Provide us with the number of rows.\n",
    "\n",
    "\n",
    "* **bytes 12-15:** Provide us with the number of columns.\n",
    "\n",
    "- - -\n",
    "\n",
    "The function `get_int` is used to help convert these important bytes to the integers we need.\n",
    "\n",
    "\n",
    "To handle the data from each file we read in, we can use the `frombuffer` function to parse the the data as an array.\n",
    "\n",
    "Everything after the 16th byte will represent a pixel value so we set an offset of 16. We also want to convert each value to uint8 (integer). Starting here we will take one byte at a time which will give us one pixel intensity value of a specific location.\n",
    "\n",
    "After we have this all that's left to do is to reshape the array according to whichever type we're dealing with. For example if we're dealing with an image then the array structure will look something like \n",
    "\n",
    "    test_images = [10,000, 28, 28]\n",
    "    ie. 10,000, 28x28 images\n",
    "    All of this data was obtained from bytes 4-16\n",
    "    \n",
    "Below is a helpful graphic from the official MNIST [site](http://yann.lecun.com/exdb/mnist/) to help solidify the concepts we've discussed already.\n",
    "\n",
    "<img src=\"./images/image_label_layout_1.png\" style=\"width: 500px; height: 342px; float: left;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can isolate images and labels successfully we need to seperate them into their respective sets of **train** or **test**. For this all we need to do is look at the length of each array. If the length is 10,000 then we know these images are for testing. If the length is 60,000 then we know these images are for training. We can then store each set as a key in our data dictionary data structure for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading  t10k-images-idx3-ubyte\n",
      "Reading  t10k-labels-idx1-ubyte\n",
      "Reading  train-images-idx3-ubyte\n",
      "Reading  train-labels-idx1-ubyte\n"
     ]
    }
   ],
   "source": [
    "import os,codecs,numpy\n",
    "\n",
    "# The path containing the extracted MNIST data\n",
    "datapath = './Data/MNISTData/'\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(datapath)\n",
    "\n",
    "# Convert 4 bytes to an integer\n",
    "def get_int(b):   \n",
    "    return int(codecs.encode(b, 'hex'), 16)\n",
    "\n",
    "# Dictionary to store the images and labels\n",
    "data_dict = {}\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for file in files:\n",
    "    # Make sure we only read files ending with 'ubyte'\n",
    "    if file.endswith('ubyte'):  \n",
    "        # Verify we are reading the correct file\n",
    "        print('Reading ', file)\n",
    "        \n",
    "        # Define file path to read from\n",
    "        with open (datapath + file,'rb') as f:\n",
    "            # Store read data\n",
    "            data = f.read() \n",
    "            # bytes 0-3 are the 'Magic Number': defines whether the type is an image or label \n",
    "            type = get_int(data[:4])\n",
    "            # bytes 4-7 define the length of the array (Dimension 0)\n",
    "            length = get_int(data[4:8])\n",
    "            \n",
    "            if (type == 2051):\n",
    "                # Set the category as images\n",
    "                category = 'images'\n",
    "                # Bytes 8-11 define the number of rows (Dimension 1)\n",
    "                num_rows = get_int(data[8:12])  \n",
    "                # Bytes 12-15 define the number of columns (Dimension 2)\n",
    "                num_cols = get_int(data[12:16])\n",
    "                # Read the pixel values as integers\n",
    "                parsed = numpy.frombuffer(data,dtype = numpy.uint8, offset = 16)  \n",
    "                # Reshape the array as [number of samples * height * width]      \n",
    "                parsed = parsed.reshape(length, num_rows, num_cols)       \n",
    "            \n",
    "            elif(type == 2049):\n",
    "                # Set the category as images\n",
    "                category = 'labels'\n",
    "                # Convert the label values to integers\n",
    "                parsed = numpy.frombuffer(data, dtype = numpy.uint8, offset = 8) \n",
    "                # Reshape the array as the number of samples (length)      \n",
    "                parsed = parsed.reshape(length)\n",
    "                \n",
    "            # Separate images/labels into their respective sets\n",
    "            # The test set contains 10,000 examples\n",
    "            if (length == 10000):\n",
    "                set = 'test'\n",
    "            # The training set contains 60,000 examples\n",
    "            elif (length == 60000):\n",
    "                set = 'train'\n",
    "                \n",
    "            # Save the NumPy array to the corresponding key\n",
    "            data_dict[set + '_' + category] = parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure our data dictionary was saved correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test_images', 'test_labels', 'train_images', 'train_labels'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['test_images'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['test_labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['train_images'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['train_labels'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, as we can see, all of our data is now nicely packaged into our dictionary data structure, which is a simple key:value pair store of "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
