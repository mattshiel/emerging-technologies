{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the MNIST Dataset\n",
    "\n",
    "It is a dataset of images of handwritten digits. The dataset contains a training set of 60,000 examples, and a test set of 10,000 examples. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1.\n",
    "\n",
    "The handwritten digits themselves were written by high school students and employees of the United States Census Bureau; divided into Special Database 1 and Special Database 3 respectively; this combination of two NIST databases are what form the MNIST dataset.\n",
    "\n",
    "# Reading the MNIST Dataset \n",
    "\n",
    "In this section I'm going to be showing you how to read the MNIST dataset into memory from scratch. This is often done in one or two lines if using a library like Tensorflow or Keras eg.\n",
    "\n",
    "```\n",
    "from keras.datasets import mnist\n",
    "# get the dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "```\n",
    "\n",
    "Pre-configured data files are fantastic and they save a lot of time, however it's important that we know how to prepare and load databases ourselves.\n",
    "\n",
    "I tried to organize this Notebook into easy to digest sections:\n",
    "\n",
    "1. Download and extract the four MNIST g-zipped files\n",
    "2. Process the encoded images\n",
    "3. Save the images to a dictionary\n",
    "4. Store dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Downloading and Extracting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading  train-images-idx3-ubyte.gz\n",
      "Downloading  train-labels-idx1-ubyte.gz\n",
      "Downloading  t10k-images-idx3-ubyte.gz\n",
      "Downloading  t10k-labels-idx1-ubyte.gz\n",
      "All files are available\n"
     ]
    }
   ],
   "source": [
    "# Adapted from https://github.com/Ghosh4AI/Data-Processors/blob/master/MNIST/MNIST_Loader.ipynb\n",
    "\n",
    "import os,urllib.request\n",
    "\n",
    "# The path where the necessary files are downloaded to\n",
    "datapath = './Data/MNISTData/'  \n",
    "\n",
    "# Create the directory if it doesn't already exist\n",
    "if not os.path.exists(datapath):\n",
    "    os.makedirs(datapath)\n",
    "\n",
    "# The necessary download URLS for the training and testing images/labels\n",
    "urls = ['http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
    "       'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
    "       'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
    "       'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz']\n",
    "\n",
    "# Loop through each URL \n",
    "for url in urls:\n",
    "    # GET FILENAME\n",
    "    filename = url.split('/')[-1]\n",
    "    \n",
    "    if os.path.exists(datapath+filename):\n",
    "        print(filename, ' already exists')  # CHECK IF FILE EXISTS\n",
    "    else:\n",
    "        print('Downloading ',filename)\n",
    "        # Adapted from https://docs.python.org/3/howto/urllib2.html\n",
    "        urllib.request.urlretrieve (url, datapath+filename)\n",
    "print('All files are available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting  t10k-images-idx3-ubyte.gz\n",
      "Extracting  train-images-idx3-ubyte.gz\n",
      "Extracting  train-labels-idx1-ubyte.gz\n",
      "Extracting  t10k-labels-idx1-ubyte.gz\n",
      "Extraction Complete\n",
      "\n",
      "Removing  t10k-images-idx3-ubyte.gz\n",
      "Removing  train-images-idx3-ubyte.gz\n",
      "Removing  train-labels-idx1-ubyte.gz\n",
      "Removing  t10k-labels-idx1-ubyte.gz\n",
      "All archives removed\n"
     ]
    }
   ],
   "source": [
    "import os,gzip,shutil\n",
    "\n",
    "# The path containing the MNIST data\n",
    "datapath = './Data/MNISTData/'  \n",
    "\n",
    "# List all folders in the directory\n",
    "files = os.listdir(datapath)\n",
    "\n",
    "for file in files:\n",
    "    if file.endswith('gz'):\n",
    "        print('Extracting ',file)\n",
    "        with gzip.open(datapath + file, 'rb') as file_in:\n",
    "            with open(datapath + file.split('.')[0], 'wb') as file_out:\n",
    "                shutil.copyfileobj(file_in, file_out)\n",
    "print('Extraction Complete\\n')\n",
    "\n",
    "# Clean up and remove the gz folders, run twice to remove all files\n",
    "for file in files:\n",
    "    print('Removing ', file)\n",
    "    os.remove(datapath + file)\n",
    "print ('All archives removed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
