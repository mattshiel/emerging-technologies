{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the MNIST Dataset\n",
    "\n",
    "It is a dataset of images of handwritten digits. The dataset contains a training set of 60,000 examples, and a test set of 10,000 examples. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1.\n",
    "\n",
    "The handwritten digits themselves were written by high school students and employees of the United States Census Bureau; divided into Special Database 1 and Special Database 3 respectively; this combination of two NIST databases are what form the MNIST dataset.\n",
    "\n",
    "# Reading the MNIST Dataset \n",
    "\n",
    "In this section I'm going to be showing you how to read the MNIST dataset into memory from scratch. This is often done in one or two lines if using a library like Tensorflow or Keras eg.\n",
    "\n",
    "```\n",
    "from keras.datasets import mnist\n",
    "# get the dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "```\n",
    "\n",
    "Pre-configured data files are fantastic and they save a lot of time, however it's important that we know how to prepare and load databases ourselves.\n",
    "\n",
    "I tried to organize this Notebook into easy to digest sections:\n",
    "\n",
    "1. Download and extract the four MNIST g-zipped files\n",
    "2. Process the encoded images\n",
    "3. Save the images to a dictionary\n",
    "4. Store dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Downloading and Extracting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading  train-images-idx3-ubyte.gz\n",
      "Downloading  train-labels-idx1-ubyte.gz\n",
      "Downloading  t10k-images-idx3-ubyte.gz\n",
      "Downloading  t10k-labels-idx1-ubyte.gz\n",
      "All files are available\n"
     ]
    }
   ],
   "source": [
    "# Adapted from https://github.com/Ghosh4AI/Data-Processors/blob/master/MNIST/MNIST_Loader.ipynb\n",
    "\n",
    "import os,urllib.request\n",
    "\n",
    "# The path where the necessary files are downloaded to\n",
    "datapath = './Data/MNISTData/'  \n",
    "\n",
    "# Create the directory if it doesn't already exist\n",
    "if not os.path.exists(datapath):\n",
    "    os.makedirs(datapath)\n",
    "\n",
    "# The necessary download URLS for the training and testing images/labels\n",
    "urls = ['http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
    "       'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
    "       'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
    "       'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz']\n",
    "\n",
    "# Loop through each URL \n",
    "for url in urls:\n",
    "    # Get individual filenames\n",
    "    filename = url.split('/')[-1]\n",
    "    \n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(datapath + filename):\n",
    "        print(filename, ' already exists')\n",
    "    else:\n",
    "        print('Downloading ', filename)\n",
    "        # Copy object specified in the URL to the local file\n",
    "        urllib.request.urlretrieve (url, datapath + filename)\n",
    "        \n",
    "print('All files are available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting  t10k-images-idx3-ubyte.gz\n",
      "Extracting  train-images-idx3-ubyte.gz\n",
      "Extracting  train-labels-idx1-ubyte.gz\n",
      "Extracting  t10k-labels-idx1-ubyte.gz\n",
      "Extraction Complete\n",
      "\n",
      "Removing  t10k-images-idx3-ubyte.gz\n",
      "Removing  train-images-idx3-ubyte.gz\n",
      "Removing  train-labels-idx1-ubyte.gz\n",
      "Removing  t10k-labels-idx1-ubyte.gz\n",
      "All archives removed\n"
     ]
    }
   ],
   "source": [
    "import os,gzip,shutil\n",
    "\n",
    "# The path containing the MNIST data\n",
    "datapath = './Data/MNISTData/'  \n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(datapath)\n",
    "\n",
    "# Read each file\n",
    "for file in files:\n",
    "    if file.endswith('gz'):\n",
    "        print('Extracting ',file)   \n",
    "        # Open the folder and read the file to copy from\n",
    "        with gzip.open(datapath + file, 'rb') as file_in:\n",
    "            # Define the new file to copy to (read and drop the .gz extension)\n",
    "            with open(datapath + file.split('.')[0], 'wb') as file_out:\n",
    "                # copy the object contents to the new file\n",
    "                shutil.copyfileobj(file_in, file_out)\n",
    "print('Extraction Complete\\n')\n",
    "\n",
    "# Clean up and remove the gz folders, run twice to remove all files\n",
    "for file in files:\n",
    "    print('Removing ', file)\n",
    "    os.remove(datapath + file)\n",
    "print ('All archives removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Processing the Encoded Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is processing the images. The primary challenge here is to isolate images from labels which we can do using the _magic number_, don't worry that will be explained below. \n",
    "\n",
    "Each file has a _magic number_, which is an integer. Integers in the files are stored in the MSB first (high endian) format. The first four bytes must therefore be converted to an integer, which will either be 2049 for labels or 2051 for images. Remember, a 32 bit integer is a 4 byte integer.\n",
    "\n",
    "<img src=\"./images/image_label_layout_1.png\" style=\"width: 500px; height: 342px; float: left;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading  t10k-images-idx3-ubyte\n",
      "Reading  t10k-labels-idx1-ubyte\n",
      "Reading  train-images-idx3-ubyte\n",
      "Reading  train-labels-idx1-ubyte\n"
     ]
    }
   ],
   "source": [
    "import os,codecs,numpy\n",
    "\n",
    "# The path containing the extracted MNIST data\n",
    "datapath = './Data/MNISTData/'\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(datapath)\n",
    "\n",
    "# Convert 4 bytes to an integer\n",
    "def get_int(b):   \n",
    "    return int(codecs.encode(b, 'hex'), 16)\n",
    "\n",
    "# Dictionary to store the images and labels\n",
    "data_dict = {}\n",
    "\n",
    "for file in files:\n",
    "    # Traverse all 'ubyte' files\n",
    "    if file.endswith('ubyte'):  \n",
    "        print('Reading ', file)\n",
    "        \n",
    "        # Define file path to read from\n",
    "        with open (datapath + file,'rb') as f:\n",
    "            # Store read data\n",
    "            data = f.read() \n",
    "            # bytes 0-3 are the 'Magic Number': defines whether the type is an image or label \n",
    "            type = get_int(data[:4])\n",
    "            # bytes 4-7 define the length of the array (Dimension 0)\n",
    "            length = get_int(data[4:8])\n",
    "            \n",
    "            if (type == 2051):\n",
    "                # Set the category as images\n",
    "                category = 'images'\n",
    "                # Bytes 8-11 define the number of rows (Dimension 1)\n",
    "                num_rows = get_int(data[8:12])  \n",
    "                # Bytes 12-15 define the number of columns (Dimension 2)\n",
    "                num_cols = get_int(data[12:16])\n",
    "                # Read the pixel values as integers\n",
    "                parsed = numpy.frombuffer(data,dtype = numpy.uint8, offset = 16)  \n",
    "                # Reshape the array as [number of samples * height * width]      \n",
    "                parsed = parsed.reshape(length, num_rows, num_cols)       \n",
    "            \n",
    "            elif(type == 2049):\n",
    "                # Set the category as images\n",
    "                category = 'labels'\n",
    "                # Convert the label values to integers\n",
    "                parsed = numpy.frombuffer(data, dtype = numpy.uint8, offset = 8) \n",
    "                # Reshape the array as the number of samples (length)      \n",
    "                parsed = parsed.reshape(length)                      \n",
    "            \n",
    "            if (length == 10000):\n",
    "                set = 'test'\n",
    "            elif (length == 60000):\n",
    "                set = 'train'\n",
    "                \n",
    "            # Save the NumPy array to the corresponding key\n",
    "            data_dict[set + '_' + category] = parsed  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
